# -*- coding: utf-8 -*-
"""nlp_pretrainedmodels.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HNI0XwK4078vkTRRjL_-DXeNfZmRTQnD
"""

from sklearn.preprocessing import normalize

!python -m spacy download en_core_web_trf

import pandas as pd
import torch
from transformers import (
    AutoTokenizer,
    AutoModel,
    pipeline
)
import spacy
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import spacy
from tqdm import tqdm

file_path = "/content/sustainable_products_dataset_expanded.csv"
df = pd.read_csv(file_path)
df.head()

required_cols = ['product_id','raw_text','category','material','certifications','brand','badges']
df = df[required_cols].dropna(subset=['raw_text']).reset_index(drop=True)
print(f"Dataset loaded with {len(df)} rows.")

def get_mean_embedding(texts, model_name):
    """
    Generates mean embeddings for each sentence using a pretrained transformer model.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    embeddings = []

    for text in tqdm(texts, desc=f"Embedding with {model_name}"):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)
        with torch.no_grad():
            outputs = model(**inputs)
        last_hidden_state = outputs.last_hidden_state.mean(dim=1)
        embeddings.append(last_hidden_state.squeeze().numpy())
    return embeddings

def evaluate_embeddings(model_embeddings, label_texts, model_name):
    """Evaluate cosine similarity + simple accuracy between embeddings and labels."""
    label_embeddings = get_mean_embedding(label_texts, model_name)
    cosine_scores = [cosine_similarity([e], [l])[0][0] for e, l in zip(model_embeddings, label_embeddings)]

    # Accuracy = percentage of scores above threshold
    threshold = 0.8
    accuracy = sum(1 for s in cosine_scores if s >= threshold) / len(cosine_scores)
    avg_cosine = sum(cosine_scores) / len(cosine_scores)
    return avg_cosine, accuracy

def run_distilbert(df):
    print("\n=== Running DistilBERT ===")
    model_name = "distilbert-base-uncased"
    embeddings = get_mean_embedding(df["raw_text"].tolist(), model_name)
    label_texts = [" ".join(map(str, row)) for row in df[['category','material','certifications','brand','badges']].values]
    score, acc = evaluate_embeddings(embeddings, label_texts)
    print(f"DistilBERT - Cosine: {score:.4f}, Accuracy: {acc:.4f}")
    return score, acc

def run_albert(df):
    print("\n=== Running ALBERT (Improved) ===")
    model_name = "albert-base-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    embeddings = []
    for text in tqdm(df["raw_text"], desc="Embedding with ALBERT"):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
        with torch.no_grad():
            outputs = model(**inputs)
        # Mean pooling + normalization
        mean_emb = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        embeddings.append(mean_emb)

    # Normalize embeddings for stability in cosine computation
    embeddings = normalize(embeddings)

    # Prepare label texts
    label_texts = [" ".join(map(str, row)) for row in df[['category', 'material', 'certifications', 'brand', 'badges']].values]
    score, acc = evaluate_embeddings(embeddings, label_texts)
    print(f"ALBERT - Cosine: {score:.4f}, Accuracy: {acc:.4f}")
    return score, acc

def run_electra(df):
    print("\n=== Running ELECTRA (Improved) ===")
    model_name = "google/electra-base-discriminator"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)

    embeddings = []
    for text in tqdm(df["raw_text"], desc="Embedding with ELECTRA"):
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
        with torch.no_grad():
            outputs = model(**inputs)
        # Mean pooling + normalization
        mean_emb = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        embeddings.append(mean_emb)

    # Normalize embeddings for consistency
    embeddings = normalize(embeddings)

    # Prepare label texts
    label_texts = [" ".join(map(str, row)) for row in df[['category', 'material', 'certifications', 'brand', 'badges']].values]
    score, acc = evaluate_embeddings(embeddings, label_texts)
    print(f"ELECTRA - Cosine: {score:.4f}, Accuracy: {acc:.4f}")
    return score, acc

def run_xlnet(df):
    print("\n=== Running XLNet ===")
    model_name = "xlnet-large-cased"
    embeddings = get_mean_embedding(df["raw_text"].tolist(), model_name)
    label_texts = [" ".join(map(str, row)) for row in df[['category','material','certifications','brand','badges']].values]
    score, acc = evaluate_embeddings(embeddings, label_texts)
    print(f"XLNet - Cosine: {score:.4f}, Accuracy: {acc:.4f}")
    return score, acc

def run_spacy_ner(df):
    print("\n=== Running spaCy NER ===")
    nlp = spacy.load("en_core_web_trf")
    detected = 0
    for text in tqdm(df["raw_text"], desc="spaCy Processing"):
        doc = nlp(text)
        if len(doc.ents) > 0:
            detected += 1
    accuracy = detected / len(df)
    print(f"spaCy NER - Entity Detection Accuracy: {accuracy:.4f}")
    return accuracy

results = {}

results["DistilBERT"] = run_distilbert(df)

results["ALBERT"] = run_albert(df)

results["ELECTRA"] = run_electra(df)

results["XLNet"] = run_xlnet(df)

results["spaCy NER"] = (None, run_spacy_ner(df))

comparison_df = pd.DataFrame([
    {"Model": name, "Cosine Similarity": vals[0] if vals[0] else 0, "Accuracy": vals[1]}
    for name, vals in results.items()
])
print("Model Comparision")
print(comparison_df)

import matplotlib.pyplot as plt
import numpy as np

# Create figure
plt.figure(figsize=(10, 6))

# X-axis positions
x = np.arange(len(comparison_df["Model"]))
width = 0.35  # width of the bars

# Plot bars
plt.bar(x - width/2, comparison_df["Cosine Similarity"], width, label="Cosine Similarity")
plt.bar(x + width/2, comparison_df["Accuracy"], width, label="Accuracy")

# Labels and title
plt.xlabel("Model")
plt.ylabel("Scores")
plt.title("Model Comparison: Cosine Similarity vs Accuracy")
plt.xticks(x, comparison_df["Model"])
plt.legend()

# Add values on bars
for i, val in enumerate(comparison_df["Cosine Similarity"]):
    plt.text(i - width/2, val + 0.01, f"{val:.2f}", ha='center', fontsize=9)
for i, val in enumerate(comparison_df["Accuracy"]):
    plt.text(i + width/2, val + 0.01, f"{val:.2f}", ha='center', fontsize=9)

plt.tight_layout()
plt.show()

comparison_df.to_csv("/content/model_comparison_with_accuracy.csv", index=False)
print("\nSaved comparison results")